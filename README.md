# Spark ETL Test

This repo includes some initial Proof of Concept work on [Spark][Spark] ETL
jobs for Zest Health.

[Spark][Spark] has been selected due to its flexibilty of running standalone as
an open source package, as well as its integration into the [AWS Glue][AWS
Glue] service. Out of the box, [Spark][Spark] has bindings for Java, Scala,
Python, and R.

Microsoft has also created C# bindings in a project called [Mobius][Mobius],
but it appears to not support .Net Core at the moment:
  
  https://github.com/Microsoft/Mobius/issues/666

## Setup

### Download Spark

To keep the repo size down, we do not include the [Spark][Spark] installation. Work so
far has been tested with [Spark][Spark] release 2.2.1, with the pre-build Hadoop 2.7 and
later package. You will need to download and unzip this file into the root of
this repo:

  http://mirrors.sorengard.com/apache/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz

In case this mirror is unavailable, the download can be selected here:

  https://spark.apache.org/downloads.html

### Start Spark master and worker processes

Once you have [Spark][Spark] downloaded/unzipped, you can use the following script to start
the master and worker processes:

```bash
$ bin/start
```

To stop the processes, you can use the following script:

```bash
$ bin/stop
```

The [Spark][Spark] Master UI is available at
[http://localhost:8080/](http://localhost:8080/).

## Python Eligibility File ETL Process

To test the python Eligibility file ETL process, run the following script:

```bash
$ bin/run-eligibility-file-py.sh
```

This will take the eligibility-sample.txt and convert it into JSON files
located in `eligibility-sample-${DATESTAMP}/`. It currently partitions the data
into a file for every 100 members. This is arbitrary and can be changed. Each
file, however, is not a valid JSON document, and contains one JSON "document"
per line, where each "document" represents a single member.

Realistically, the production [Spark][Spark] job will likely invoke an API instead of
saving JSON files, but that will still rely on a format like JSON for
submission, so this work _should_ be relevant to the final implementation. It's
also for this reason that we're ignoring the fact that each file generated by
[Spark][Spark] is not a valid JSON document. If we need to save to a file in
the production use-case, we can figure that out at that time.

[Spark]: https://spark.apache.org/
[AWS Glue]: https://aws.amazon.com/glue/
[Mobius]: https://github.com/Microsoft/Mobius
